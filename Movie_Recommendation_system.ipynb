{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4B3JmyapG9JyYQLwpDDIp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Obi-chuks/Recommendation-System/blob/main/Movie_Recommendation_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# HYBRID RECOMMENDATION SYSTEM\n",
        "# Final Evaluation Score: 24.18%\n",
        "# Strategy: SVD + Temporal Decay + Popularity Penalty\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import cloudpickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import normalize\n",
        "from collections import defaultdict\n",
        "\n",
        "# Setup logging to track training milestones in the console\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 1. TRAINING ENGINE\n",
        "# ==========================================\n",
        "class HybridSVD:\n",
        "    \"\"\"\n",
        "    The main engine that learns from user behavior.\n",
        "    It combines Matrix Factorization with time-based importance weights.\n",
        "    \"\"\"\n",
        "    REQUIRED_COLS = {\"userId\", \"movieId\", \"rating\"}\n",
        "\n",
        "    def __init__(self, n_components=60):\n",
        "        # n_components=60: The number of 'hidden' features the model extracts.\n",
        "        # Too low = misses nuances; Too high = overfits to noise.\n",
        "        self.n_components = n_components\n",
        "        self.model        = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "        self.user_factors = None\n",
        "        self.item_factors = None\n",
        "        self.u_map        = {}  # Maps internal indices back to original UserIDs\n",
        "        self.i_map        = {}  # Maps internal indices back to original MovieIDs\n",
        "        self.u_inv_map    = {}\n",
        "        self.pop_counts   = {}  # Stores movie popularity (probability of appearance)\n",
        "        self._sparse_mx   = None\n",
        "        self._penalty_vec = None\n",
        "\n",
        "    def fit(self, interactions):\n",
        "        \"\"\"\n",
        "        Trains the model by centering ratings, applying recency weights,\n",
        "        and performing Singular Value Decomposition (SVD).\n",
        "        \"\"\"\n",
        "        missing = self.REQUIRED_COLS - set(interactions.columns)\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "        interactions = interactions.copy()\n",
        "        logger.info(\"Training SVD (n_components=%d) on %d rows...\",\n",
        "                    self.n_components, len(interactions))\n",
        "\n",
        "        # --- STEP A: USER CENTERING ---\n",
        "        # Normalizes the data: ensures 'easy raters' and 'harsh critics'\n",
        "        # are measured on the same relative scale.\n",
        "        user_means = interactions.groupby(\"userId\")[\"rating\"].mean()\n",
        "        interactions[\"centered_rating\"] = (\n",
        "            interactions[\"rating\"] - interactions[\"userId\"].map(user_means)\n",
        "        )\n",
        "\n",
        "        # --- STEP B: RECENCY WEIGHTING (TEMPORAL DECAY) ---\n",
        "        # Uses an exponential decay formula: weight = exp(-days_ago / half_life).\n",
        "        # This solves the 'Frozen Profile' problem by prioritizing recent tastes.\n",
        "        most_recent  = interactions[\"timestamp\"].max()\n",
        "        days_ago     = (most_recent - interactions[\"timestamp\"]).dt.days.clip(lower=0)\n",
        "        half_life    = 365  # 1 year half-life\n",
        "        recency_w    = np.exp(-days_ago.values / half_life).astype(\"float32\")\n",
        "\n",
        "        # Apply the weight to the ratings\n",
        "        interactions[\"centered_rating\"] = (\n",
        "            interactions[\"centered_rating\"] * recency_w\n",
        "        )\n",
        "\n",
        "        # Map User/Item IDs to categorical codes for matrix construction\n",
        "        user_cat = interactions[\"userId\"].astype(\"category\")\n",
        "        item_cat = interactions[\"movieId\"].astype(\"category\")\n",
        "\n",
        "        self.u_map     = dict(enumerate(user_cat.cat.categories))\n",
        "        self.i_map     = dict(enumerate(item_cat.cat.categories))\n",
        "        self.u_inv_map = {v: k for k, v in self.u_map.items()}\n",
        "\n",
        "        n_users, n_items = len(self.u_map), len(self.i_map)\n",
        "\n",
        "        # Ensure we don't have more factors than data points\n",
        "        n_comp = min(self.n_components, n_users - 1, n_items - 1)\n",
        "        if n_comp != self.n_components:\n",
        "            self.n_components = n_comp\n",
        "            self.model = TruncatedSVD(n_components=n_comp, random_state=42)\n",
        "\n",
        "        # Build Sparse Matrix (CSR format) to save memory (RAM)\n",
        "        self._sparse_mx = csr_matrix(\n",
        "            (interactions[\"centered_rating\"].values,\n",
        "             (user_cat.cat.codes, item_cat.cat.codes)),\n",
        "            shape=(n_users, n_items),\n",
        "        )\n",
        "\n",
        "        # Extract latent features\n",
        "        self.user_factors = self.model.fit_transform(self._sparse_mx).astype(\"float32\")\n",
        "        self.item_factors = self.model.components_.astype(\"float32\")\n",
        "\n",
        "        # Normalize for better ranking (Cosine Similarity approach)\n",
        "        self.user_factors = normalize(self.user_factors, axis=1)\n",
        "\n",
        "        # Create a penalty vector based on global popularity to promote diversity\n",
        "        self.pop_counts   = interactions[\"movieId\"].value_counts(normalize=True).to_dict()\n",
        "        self._penalty_vec = np.array(\n",
        "            [self.pop_counts.get(self.i_map[i], 0.0) for i in range(n_items)],\n",
        "            dtype=\"float32\",\n",
        "        )\n",
        "\n",
        "        logger.info(\"Training done. %d users x %d items.\", n_users, n_items)\n",
        "        return self\n",
        "\n",
        "    def recommend_all(self, n_recommendations=10, penalty=0.15, batch_size=512):\n",
        "        \"\"\"\n",
        "        Generates recommendations in batches to prevent memory crashes.\n",
        "        Applies the 'Diversity Penalty' during the scoring phase.\n",
        "        \"\"\"\n",
        "        n_users = self.user_factors.shape[0]\n",
        "        n_items = self.item_factors.shape[1]\n",
        "        candidate_k = min(100, n_items)\n",
        "        k           = min(n_recommendations, candidate_k)\n",
        "        pv          = self._penalty_vec * penalty\n",
        "\n",
        "        recs = {}\n",
        "        for start in range(0, n_users, batch_size):\n",
        "            end    = min(start + batch_size, n_users)\n",
        "\n",
        "            # THE CORE CALCULATION: Matrix Multiplication - Popularity Penalty\n",
        "            scores = (self.user_factors[start:end] @ self.item_factors) - pv\n",
        "\n",
        "            for li in range(end - start):\n",
        "                # Seen Filter: don't recommend what the user already watched\n",
        "                seen = self._sparse_mx[start + li].indices\n",
        "                if len(seen):\n",
        "                    scores[li, seen] = -np.inf\n",
        "\n",
        "            # Get top indices\n",
        "            top = self._topk(scores, candidate_k)\n",
        "            for li in range(end - start):\n",
        "                uid = self.u_map[start + li]\n",
        "                recs[uid] = [int(self.i_map[m]) for m in top[li][:k]]\n",
        "\n",
        "        return recs\n",
        "\n",
        "    @staticmethod\n",
        "    def _topk(scores, k):\n",
        "        \"\"\"Efficient partial sort to find the highest scores.\"\"\"\n",
        "        nr, nc = scores.shape\n",
        "        k    = min(k, nc)\n",
        "        part = np.argpartition(-scores, k, axis=1)[:, :k]\n",
        "        rows = np.arange(nr)[:, None]\n",
        "        return part[rows, np.argsort(-scores[rows, part], axis=1)]\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. LOCAL EVALUATOR (Metrics)\n",
        "# ==========================================\n",
        "def dcg_at_k(relevances, k=10):\n",
        "    \"\"\"Measures the gain of an item based on its position in the rank.\"\"\"\n",
        "    relevances = np.array(relevances[:k])\n",
        "    if len(relevances) == 0:\n",
        "        return 0.0\n",
        "    positions = np.arange(1, len(relevances) + 1)\n",
        "    return np.sum(relevances / np.log2(positions + 1))\n",
        "\n",
        "\n",
        "def evaluate(model_recs: dict, test_interactions: pd.DataFrame,\n",
        "             all_interactions: pd.DataFrame,\n",
        "             n_recs: int = 10, relevance_threshold: float = 3.5):\n",
        "    \"\"\"\n",
        "    Evaluates the model on: NDCG, Precision, Recall, HitRate, and Catalog Coverage.\n",
        "    \"\"\"\n",
        "    eval_users = set(model_recs.keys())\n",
        "\n",
        "    # Filter for users who gave high ratings in the test set\n",
        "    relevant = (\n",
        "        test_interactions[\n",
        "            (test_interactions[\"rating\"] >= relevance_threshold) &\n",
        "            (test_interactions[\"userId\"].isin(eval_users))\n",
        "        ]\n",
        "        .groupby(\"userId\")[\"movieId\"]\n",
        "        .apply(set)\n",
        "        .to_dict()\n",
        "    )\n",
        "\n",
        "    if not relevant:\n",
        "        return None\n",
        "\n",
        "    ndcg_scores, prec_scores, rec_scores, hit_scores = [], [], [], []\n",
        "    all_recommended = set()\n",
        "\n",
        "    for user_id, true_items in relevant.items():\n",
        "        recs = model_recs.get(user_id, [])[:n_recs]\n",
        "        if not recs:\n",
        "            ndcg_scores.append(0.0); prec_scores.append(0.0)\n",
        "            rec_scores.append(0.0); hit_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        hits      = [1 if r in true_items else 0 for r in recs]\n",
        "        n_hits    = sum(hits)\n",
        "        n_true    = len(true_items)\n",
        "\n",
        "        ideal     = [1] * min(n_true, n_recs)\n",
        "        idcg      = dcg_at_k(ideal, n_recs)\n",
        "        ndcg      = dcg_at_k(hits, n_recs) / idcg if idcg > 0 else 0.0\n",
        "\n",
        "        ndcg_scores.append(ndcg)\n",
        "        prec_scores.append(n_hits / n_recs)\n",
        "        rec_scores.append(n_hits / n_true if n_true > 0 else 0.0)\n",
        "        hit_scores.append(1.0 if n_hits > 0 else 0.0)\n",
        "        all_recommended.update(recs)\n",
        "\n",
        "    total_items = all_interactions[\"movieId\"].nunique()\n",
        "    coverage    = len(all_recommended) / total_items if total_items > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"NDCG@10\":      round(np.mean(ndcg_scores) * 100, 2),\n",
        "        \"Precision@10\": round(np.mean(prec_scores) * 100, 2),\n",
        "        \"Recall@10\":    round(np.mean(rec_scores) * 100, 2),\n",
        "        \"HitRate@10\":   round(np.mean(hit_scores) * 100, 2),\n",
        "        \"Coverage\":     round(coverage * 100, 2),\n",
        "        \"Combined\":     round((0.25 * np.mean(ndcg_scores) + 0.25 * np.mean(prec_scores) +\n",
        "                               0.20 * np.mean(rec_scores) + 0.15 * np.mean(hit_scores) +\n",
        "                               0.15 * coverage) * 100, 2),\n",
        "        \"_n_eval_users\": len(relevant)\n",
        "    }\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3. CLOSURE FACTORY (Deployment Wrapper)\n",
        "# ==========================================\n",
        "def make_model(precomputed_lists: dict, default_list: list):\n",
        "    \"\"\"\n",
        "    Ensures 'model.recommend(uid)' works without needing complex libraries\n",
        "    on the evaluation server, preventing environment crashes.\n",
        "    \"\"\"\n",
        "    _pre = precomputed_lists\n",
        "    _def = default_list\n",
        "\n",
        "    def recommend(user_id, n_recommendations=10):\n",
        "        k    = int(n_recommendations)\n",
        "        hits = _pre.get(user_id)\n",
        "        if hits is not None:\n",
        "            if k <= len(hits):\n",
        "                return hits[:k]\n",
        "            # Backfill with popular movies if recommendations are short\n",
        "            seen  = set(hits)\n",
        "            extra = [x for x in _def if x not in seen]\n",
        "            return (hits + extra)[:k]\n",
        "        return _def[:k] # Cold-start fallback\n",
        "\n",
        "    recommend.recommend = recommend\n",
        "    return recommend\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 4. DATA LOADING & SPLITTING\n",
        "# ==========================================\n",
        "def load_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "    # Fix '2003-0' formatting errors in CSV\n",
        "    df[\"timestamp\"] = df[\"timestamp\"].astype(str).str.replace(r\"-0$\", \"-01-01\", regex=True)\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "    return df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
        "\n",
        "def temporal_split(df, train_ratio=0.8):\n",
        "    \"\"\"Splits by time: 80% past for training, 20% future for testing.\"\"\"\n",
        "    cutoff = int(len(df) * train_ratio)\n",
        "    train, test = df.iloc[:cutoff].copy(), df.iloc[cutoff:].copy()\n",
        "\n",
        "    # Leakage check: make sure the model isn't seeing future data early\n",
        "    if (test[\"timestamp\"] < train[\"timestamp\"].max()).sum() > 0:\n",
        "        logger.warning(\"⚠️ Temporal leakage detected!\")\n",
        "    else:\n",
        "        logger.info(\"✅ No temporal leakage detected.\")\n",
        "    return train, test\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 5. EXECUTION\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    raw_df = load_data(\"interactions_train.csv\")\n",
        "\n",
        "    # Local simulation\n",
        "    train_val, test_val = temporal_split(raw_df, train_ratio=0.8)\n",
        "    h_val = HybridSVD(n_components=60).fit(train_val)\n",
        "    recs_val = h_val.recommend_all(n_recommendations=10, penalty=0.15, batch_size=256)\n",
        "    scores = evaluate(recs_val, test_val, raw_df)\n",
        "\n",
        "    print(f\"\\nEstimate: {scores['Combined']}% | NDCG: {scores['NDCG@10']}% | Coverage: {scores['Coverage']}%\")"
      ],
      "metadata": {
        "id": "maDOeYVS7CG4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}